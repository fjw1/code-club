{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Dynamic Programing\n",
    "\n",
    "After the previous Value Function Iteration (VFI) notebooks it's time for talking discrete dynamic programming at large.\n",
    "\n",
    "The setup here is different from the previous problem (see VFI 1.5), where we took a [monte carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) approach to deal with uncertainty - we took a sample of random numbers (large enough) from the distribution of shocks and then took the mean. That way we approximated the value of the expectation operator (up to an error).\n",
    "\n",
    "There is another way of taking expectations \n",
    "\n",
    "This notebook is to be used after reading the first part (up until *Example: A Growth Model*) of the [quantecon lecture](https://lectures.quantecon.org/py/discrete_dp.html) on dynamic programming. Read it in your own (if you are not seeing this in class) first, making sure you follow fairly well what it says. The more difficult bits (value function interation) should be solid from our previous notebooks.\n",
    "\n",
    "You'll need numba, so install it now before proceeding. The folowing cell should execute without problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division  # Not needed for Python 3.x\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import quantecon as qe\n",
    "from numba import jit\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 A cookie problem\n",
    "\n",
    "The aim of this bit is to provide you with a reference guide so you cna implement your own versions in the future.\n",
    "\n",
    "The problem follows [the growth model example](https://lectures.quantecon.org/py/discrete_dp.html#example-a-growth-model) of quantecon.\n",
    "\n",
    "Suppose we live in a world where cookies are the only commodity that matters.\n",
    "\n",
    "Every night, the cookie fairies bring a random ammount of cookies to your door. The amount of cookies ($y$) received is distributed uniformly from 0 to B:\n",
    "\n",
    "$$ y \\sim U[0,B] $$\n",
    "\n",
    "So chances of getting 5 cookies are the same as taking 0.\n",
    "\n",
    "You are endowed with a cookie jar that can hold at maximum M cookies.\n",
    "\n",
    "You wake up in the morning, take your \"income\" cookies and put them together with your exisiting savings in the jar.\n",
    "\n",
    "Your goal then is to decide how many cookies out of your income ($y$) you want to save ($a$) and how many you want to consume ($c$) today (so that $y = c+a$). Consumed cookies give you a unitily of $u(c)$ and you discount the future by $\\beta$ each period.\n",
    "\n",
    "Let's make utility exponential for this case so that $u(c)=c^{\\alpha}$.\n",
    "\n",
    "> **Task 1:** Following the [quantecon](https://lectures.quantecon.org/py/discrete_dp.html#example-a-growth-model) exposition, explain with your own words what is the state variable, state space, action space, reward function and transition probabilities. No reading the math, but explaining with words.\n",
    "\n",
    "> Then use them to add comments to the setting cell below explaining what each line represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M=10             # ???\n",
    "B=5              # ???\n",
    "n = B + M + 1    # ???\n",
    "m = M + 1        # ???\n",
    "alpha=0.5        # ???\n",
    "beta=0.9         # ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use discrete markov chains to incorporate the random component of $y$ into our decisions. For that we need to code the matrix algebra version of this problem:\n",
    "\n",
    "$$ v(s) = \\max_a r(s,a) + \\beta \\sum_{s' \\in S}Q(s,a,s') v(s')$$\n",
    "\n",
    "- $Q$ is going to be a **tri**dimensional matrix that relates states today, actions today and states tomorrow.\n",
    "\n",
    "- $r(s,a)$ is going to be a matrix of utilitiesvalues, for each state-value pair $(s,a)$: $u(s-a)$. We are going to \"cheat\" by giving a utility of $-\\infty$ to the unfeasible values (say you have 0 cookie in the jar and you receive 1: You can only save either 1 or 0. If you choose to save more then you get minus infinity utility).\n",
    "\n",
    "That's why we are going to code them in the following way (make sure you understand it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = np.empty((n, m))\n",
    "Q = np.zeros((n, m, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to fill up the matrices.\n",
    "\n",
    "> **Task 2:** Complete the code below. You can do this in easy mode (go back to quantecon and copy the bits you need) or in hard mode: pen and paper if needed and coding it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reward matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Markow transition matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the cells below to print the matrices and check we did the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can plot them too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(R.T)\n",
    "plt.xlabel('action $a$')\n",
    "plt.ylabel('reward $u(a)$')\n",
    "plt.title('Each line represents a state $s$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Solving for cookies: home made solution\n",
    "\n",
    "We could go ahead and use the quantecon package for this problem, but I think it would be more useful if we know what are we doing and code the algorithms ourselves. If you don't feel adventurous you can skip to the next section.\n",
    "\n",
    "First let's solve it by VFI. We will have:\n",
    "\n",
    "$$ V_{t+1} = \\max_a R + \\beta Q V_{t}$$\n",
    "\n",
    "Our algorithm will be as follows:\n",
    "\n",
    "- We give a initial guess $V_0$ for $V_{t}$. We will have a resulting $n$ x $m$ matrix (call it *RQV*)\n",
    "- For each row of *RQV*, choose the column that gives the maximum value. This is the $\\max_a$ step.\n",
    "- Given this choice, calculate the vector $V_{t+1}$ (put all of the maximum values together in one vector)\n",
    "- Calculate the distance between $V_{t+1}$ and $V_{t}$\n",
    "- If this distance is greater than some tolerance, set our guess vector $V_{t} = V_{t+1}$ and repeat.\n",
    "\n",
    "Let's do it slowly so we get a grasp of what's going on.\n",
    "\n",
    "**1** We set our initial guess, a vector of size $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v0 = np.ones(n)\n",
    "v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** We compute the implied RQV matrix.\n",
    "\n",
    "Notive the use of ``np.dot``: if you use ``*`` you get *pointwise* multiplication, we don't want that for $Q$ and $V$, but we want that for $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RQV = R + beta*np.dot(Q,v0)\n",
    "print RQV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** We find the column (action) that gives the maximun value of each row (state).\n",
    "\n",
    "For this we use numpy function maximum, as it allows us to chose the direction (1 for column, 0 for row).\n",
    "\n",
    "We can also record our choices into a vector (of which lenght?). Call it $p$ for policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.max(RQV,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(RQV,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1  = np.max(RQV,1)\n",
    "p1 = np.argmax(RQV,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4** In equilibirum, ``v0=v1``. Let's check the maximum difference between both after one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.max(np.abs(v1-v0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the steps, it's time to implement the algorithm.\n",
    "\n",
    "> **Task 3:** Code the Value Function Iteration algorithm, following the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "print \"Done! Distance: {}  Iterations: {}\".format(distance,it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code worked out correctly, the following cell should give you the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(range(n),v1)\n",
    "plt.xlim(0,15)\n",
    "plt.xticks(range(0,16))\n",
    "plt.ylabel('$v^*(s)$', fontsize=16)\n",
    "plt.xlabel('$s$', fontsize=16)\n",
    "plt.title('Our value function', fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(n),p1, label='cookies saved')\n",
    "plt.plot(range(n),np.arange(n)-p1, label='cookies consumed')\n",
    "plt.xlim(0,15)\n",
    "plt.xticks(range(0,16))\n",
    "plt.title('Our (discrete) policy function', fontsize=16)\n",
    "plt.ylabel('$a^*(s)$, $c*(a(s))$', fontsize=16)\n",
    "plt.xlabel('$s$', fontsize=16)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Solving for cookies: Quantecon solution\n",
    "\n",
    "That was fast! But did we got the right answer? Let's check using quantecon's tools.\n",
    "\n",
    "The following lines create an *instance* (a variable containing a problem) of the ``discreteDP`` class, then solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddp = qe.markov.DiscreteDP(R, Q, beta)\n",
    "results = ddp.solve(method='value_iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable ``results`` is a dictionary  that stores the output from the algorithm. Call ``.keys()`` to see what it recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 4:** Select the right result to store the policy function in ``p_qe`` and the value function ``v_qe``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_qe = results.v\n",
    "p_qe = results.sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we got the right answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(range(n),v1, label='homemade')\n",
    "plt.plot(range(n),v_qe, label='quant-econ')\n",
    "plt.xlim(0,15)\n",
    "plt.xticks(range(0,16))\n",
    "plt.ylabel('$v^*(s)$', fontsize=16)\n",
    "plt.xlabel('$s$', fontsize=16)\n",
    "plt.legend(loc='best', frameon=True, fontsize=14)\n",
    "plt.title('Our value function', fontsize=16)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(n),p1, label='homemade')\n",
    "plt.plot(range(n),p_qe, label='quant-econ')\n",
    "plt.xlim(0,15)\n",
    "plt.xticks(range(0,16))\n",
    "plt.title('Our (discrete) policy function', fontsize=16)\n",
    "plt.ylabel('$a^*(s)$', fontsize=16)\n",
    "plt.xlabel('$s$', fontsize=16)\n",
    "plt.legend(loc='best', frameon=True, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! But mind you, the routines of the quantecon function are more sophisticated that ours, so they solve the problem faster. Compare the number of iterations it took to get to the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Our number of iterations: {}\".format(it)\n",
    "print \"QuantEcon number of iterations: {}\".format(results.num_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Back to our growth model\n",
    "\n",
    "Can we use the tools we learnt to get to the same answer we got last week?\n",
    "\n",
    "This section goes through the answer to the first exercise of the quantecon lecture. But if you are up for a real challenge, try to do it yourself! Note that it is quite hard.\n",
    "\n",
    "The exercise constitutes and example of how to present a problem in a way you can use the quantecon routine to solve it. Which is, incidentally, what you will end up doing in coding: finding the right tools for your problem and adapting your problem  so you can use them effectively.\n",
    "\n",
    "The first step is to recode the main parameters (including the capital grid) and the analytical answer. There is no productivity shock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining Parameters\n",
    "alpha = 0.65\n",
    "f = lambda k: k**alpha\n",
    "u = np.log\n",
    "beta = 0.95\n",
    "\n",
    "# Defining grid\n",
    "grid_max = 2\n",
    "grid_size = 200\n",
    "grid = np.linspace(1e-6, grid_max, grid_size)\n",
    "\n",
    "# Exact solution (for comparison)\n",
    "ab = alpha * beta\n",
    "c1 = (np.log(1 - ab) + np.log(ab) * ab / (1 - ab)) / (1 - beta)\n",
    "c2 = alpha / (1 - ab)\n",
    "\n",
    "def v_star(k):\n",
    "    return c1 + c2 * np.log(k)\n",
    "\n",
    "true_c = (1 - alpha * beta) * grid**alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define our feasible set $A(s)$. This is going to ge all of the combinations of state (capital) and action (consumption) that yield non-negative state tomorrow (capital tomorrow).\n",
    "\n",
    "We use the ``np.where`` function to locate these values.  ``np.where`` returns two vectors: one with the row indexes and one with the column indexes. For example if our set was:\n",
    "\n",
    "```\n",
    "[[0,0,-9],\n",
    "[0,-1,26]]\n",
    "```\n",
    "\n",
    "Then ``np.where`` would return: ``[0,0,1,1],[0,1,0,2]``. The coordinates of all of the points that are greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Consumption matrix, with nonpositive consumption included\n",
    "As = f(grid).reshape(grid_size, 1) - grid.reshape(1, grid_size)\n",
    "# size 150x1, 1x150\n",
    "\n",
    "# State-action indices\n",
    "s_indices, a_indices = np.where(As > 0)\n",
    "\n",
    "# Number of state-action pairs\n",
    "L = len(s_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course ``s_indices`` and ``a_indices`` conatin the same number of elements ``L``. You can check what the variables contain using the print cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print s_indices, a_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense, right?\n",
    "\n",
    "So what are going to be our R and Q matrices? The ones corresponding to the feasible state-action pairs. For that reason, the reward matrix $R$ is going to be a vector this time (each element corresponding to a state-action) of length $1$x$L$.\n",
    "\n",
    "Now for $Q$: there is no uncertainty, so $Q$ will map action pairs $As$ to the corresponding level of capital tomorrow. So each row will map each action pair to the state it came from $A(s) \\rightarrow s$. This is because as in the no uncertainty case:\n",
    "\n",
    "$$ V(k) = \\max_{k'} {u(k') + \\beta V(k')}$$\n",
    "\n",
    "So it does make sense that the $k'$ in $V(k')$ should be the same as in $V(k)$ with probability 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = u(As[s_indices, a_indices])\n",
    "Q = np.zeros((L, grid_size))    # creates a 'sparse' matrix, with all zeros\n",
    "\n",
    "Q[np.arange(L), a_indices] = 1          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if it is confusing. This notation is used by the ``DiscreteDP`` class to solve the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddp2 = qe.markov.DiscreteDP(R, Q, beta, s_indices, a_indices)\n",
    "results2 = ddp2.solve(method='policy_iteration')\n",
    "v_qe2 = results2.v               # Value function\n",
    "p_qe2 = results2.sigma           # Optimal policy (savings)\n",
    "c_qe2 = f(grid) - grid[p_qe2]    # Optimal consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as you can see, it solves the problem correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(grid,v_qe2, label='Discrete $c^*(a_i)$')\n",
    "plt.plot(grid,v_star(grid), label='True $c^*(a_i)$')\n",
    "plt.ylim(-42, -32)\n",
    "plt.ylabel('$v^*(a_i)$', fontsize=12)\n",
    "plt.xlabel('$a_i$', fontsize=12)\n",
    "plt.legend(loc='best',fontsize=12, frameon=True)\n",
    "plt.title('Comparing the value function', fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(grid,v_qe2, label='Discrete $c^*(a_i)$')\n",
    "plt.plot(grid,v_star(grid), label='True $c^*(a_i)$')\n",
    "plt.ylim(-35.2, -34.4)\n",
    "plt.xlim(0.8,1.2)\n",
    "plt.xlabel('$a_i$', fontsize=12)\n",
    "plt.title('Comparing the value function', fontsize=14)\n",
    "plt.title('Zoom around 1', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Zoom in\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(grid,c_qe2, label='Discrete $c^*(a_i)$')\n",
    "plt.plot(grid,true_c, color='black', alpha=0.5, label='True $c^*(a_i)$')\n",
    "plt.title('Policy function (zoom around 1)', fontsize=18)\n",
    "plt.xlabel('$a_i$', fontsize=16)\n",
    "plt.ylabel('$c^*(a_i)$', fontsize=16)\n",
    "plt.xlim(0.8,1.2)\n",
    "plt.ylim(0.3,0.45)\n",
    "plt.legend(loc='best',fontsize=16, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Take aways and results\n",
    "\n",
    "As you can see, it can be tricky to work with markov chain transitions, but the speed and accuracy of results is worth it.\n",
    "\n",
    "From here, I propose you pick up on of the next exercises to do for next session:\n",
    "\n",
    "1. Implement the policy function iteration to the cookie problem. I haven't had time to talk about it this time, but it is a great opportunity to do so in the next one.\n",
    "\n",
    "2. Modify the code above to include the innovation as we saw it in VFI 1.5. This is quite a challenge, because it requires first to transform the lognromal shock into a markov transition matrix, and then specify the Q and R matrices correctly. It is great for understanding though, and certainly useful for the future.\n",
    "\n",
    "3. More fun: Read the [McCall model lecture](https://lectures.quantecon.org/py/mccall_model.html) and add to it assets - that is, a grid for assets and a consumption-saving decision. Compare the wage reservation you get to the 'hand-to-mouth' scenario.\n",
    "\n",
    "4. Play around risk aversion using different utility functions. Do you get the result you expected?\n",
    "\n",
    "And after that we will conclude our introduction to dynamic programming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
